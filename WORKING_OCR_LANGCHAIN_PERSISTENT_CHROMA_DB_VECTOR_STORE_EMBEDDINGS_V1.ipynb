{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SatadruNoob/BOLT-RAG-PIPELINE/blob/main/WORKING_OCR_LANGCHAIN_PERSISTENT_CHROMA_DB_VECTOR_STORE_EMBEDDINGS_V1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "############## CLEAN INSTALLATION ################\n",
        "# Install Python packages\n",
        "%pip install --quiet --upgrade \\\n",
        "    langchain \\\n",
        "    langchain-community \\\n",
        "    langchain-text-splitters \\\n",
        "    langgraph \\\n",
        "    langchain[mistralai] \\\n",
        "    langchain-huggingface \\\n",
        "    langchain-chroma \\\n",
        "    pypdf \\\n",
        "    chromadb \\\n",
        "    PyPDF2 \\\n",
        "    pytesseract \\\n",
        "    pdf2image\n",
        "\n",
        "# Install system packages\n",
        "!apt-get update -qq\n",
        "!apt-get install -y tesseract-ocr poppler-utils"
      ],
      "metadata": {
        "id": "Q2DNGZ-W5XA1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jJWpJl5wOh2p"
      },
      "source": [
        "RESTART RESTART RESTART RESTART RESTART RESTART RESTART RESTART RESTART RESTART RESTART RESTART"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "############## CLEAN IMPORTS ################\n",
        "# Core Python Libraries\n",
        "import os\n",
        "import time\n",
        "from typing import List, Literal, Optional, Dict  # <- List from typing\n",
        "from typing_extensions import TypedDict, List as TEList # <- Also List from typing_extensions\n",
        "from typing_extensions import TypedDict, Annotated, List\n",
        "\n",
        "# Colab-Specific\n",
        "from google.colab import drive, userdata\n",
        "from IPython.display import Image, display\n",
        "\n",
        "# PDF & OCR\n",
        "import pytesseract\n",
        "pytesseract.pytesseract.tesseract_cmd = \"/usr/bin/tesseract\"\n",
        "\n",
        "from pypdf import PdfReader\n",
        "\n",
        "# LangChain & LangGraph\n",
        "import langchain\n",
        "import langchain_community\n",
        "import langgraph  # <- Explicitly included\n",
        "print(\"LangChain:\", langchain.__version__)\n",
        "\n",
        "# LangChain Core\n",
        "from langchain import hub\n",
        "from langchain_core.documents import Document\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.vectorstores import InMemoryVectorStore\n",
        "\n",
        "# LangChain Community & Tools\n",
        "from langchain_community.document_loaders import PyPDFLoader, TextLoader\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "# LangGraph Components\n",
        "from langgraph.graph import StateGraph, START\n",
        "\n",
        "# Embeddings\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "\n",
        "# Chroma Configuration\n",
        "from chromadb.config import Settings"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q9MdGFCL6Z5b",
        "outputId": "364624fb-14c6-473d-e302-1db8623a94a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LangChain: 0.3.25\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "########## Environment Setup: LangChain + Mistral + LangSmith (Colab RAG Pipeline) ##########\n",
        "# Set LangSmith environment variables\n",
        "os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
        "os.environ[\"LANGSMITH_API_KEY\"] = userdata.get('LANGSMITH_API_KEY')\n",
        "print(\"LangSmith environment variables set successfully!\")\n",
        "\n",
        "# Set Mistral API key from Colab secrets\n",
        "os.environ[\"MISTRAL_API_KEY\"] = userdata.get('MISTRAL_API_KEY')\n",
        "\n",
        "# Initialize the Mistral chat model\n",
        "from langchain.chat_models import init_chat_model\n",
        "llm = init_chat_model(\"mistral-large-latest\", model_provider=\"mistralai\")\n",
        "print(\"Mistral AI LLM initialized successfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nOGoepZy678R",
        "outputId": "dbc8b801-4ff7-4032-9694-045580544fb2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LangSmith environment variables set successfully!\n",
            "Mistral AI LLM initialized successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "88jkopArOgd1",
        "outputId": "3a952b96-ec53-44eb-c29a-5ff26fabb3a7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CB5-fimx64E2"
      },
      "source": [
        "################################################################################"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile ocr_update.py\n",
        "#### OCR ENABLED TEXT EXTRACTION FROM PDF ########\n",
        "#########\n",
        "import os\n",
        "import hashlib\n",
        "import pytesseract\n",
        "import re\n",
        "from PyPDF2 import PdfReader\n",
        "from pdf2image import convert_from_path\n",
        "from langchain.schema import Document\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "import chromadb\n",
        "from IPython.display import display, Markdown\n",
        "\n",
        "\n",
        "def sanitize_collection_name(name: str) -> str:\n",
        "    name = name.replace(\" \", \"_\")\n",
        "    name = re.sub(r'[^a-zA-Z0-9_\\-]', '', name)\n",
        "    return name[:63]\n",
        "\n",
        "def compute_hash(content: str) -> str:\n",
        "    return hashlib.sha256(content.encode(\"utf-8\")).hexdigest()\n",
        "\n",
        "def extract_text_from_pdf_ocr(pdf_path):\n",
        "    try:\n",
        "        images = convert_from_path(pdf_path)\n",
        "        text = \"\"\n",
        "        for image in images:\n",
        "            text += pytesseract.image_to_string(image)\n",
        "        return text\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå OCR failed for {pdf_path}: {str(e)}\")\n",
        "        return \"\"\n",
        "\n",
        "def get_existing_hashes(chroma_store):\n",
        "    try:\n",
        "        results = chroma_store._collection.get(include=[\"metadatas\"])\n",
        "        return {md[\"content_hash\"] for md in results.get(\"metadatas\", []) if md and \"content_hash\" in md}\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Failed to get existing hashes: {str(e)}\")\n",
        "        return set()\n",
        "\n",
        "def ocr_and_update_chroma(doc_dir, persist_dir):\n",
        "    collection_name = sanitize_collection_name(os.path.basename(doc_dir))\n",
        "    print(f\"üóÇ Using collection name: `{collection_name}`\")\n",
        "    print(\"üîç Initializing embedding and vector store...\")\n",
        "\n",
        "    embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\n",
        "    chroma_store = Chroma(\n",
        "        client=chromadb.PersistentClient(path=persist_dir),\n",
        "        collection_name=collection_name,\n",
        "        embedding_function=embeddings,\n",
        "        persist_directory=persist_dir\n",
        "    )\n",
        "\n",
        "    existing_hashes = get_existing_hashes(chroma_store)\n",
        "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "    new_chunks = []\n",
        "    preview_shown = False\n",
        "\n",
        "    print(\"üîé Scanning and applying OCR to all PDFs...\")\n",
        "\n",
        "    for i, file in enumerate(sorted(os.listdir(doc_dir)), 1):\n",
        "        path = os.path.join(doc_dir, file)\n",
        "        if file.lower().endswith(\".pdf\"):\n",
        "            print(f\"‚ñ∂Ô∏è [{i}] Processing: {file}\")\n",
        "            text = extract_text_from_pdf_ocr(path)\n",
        "\n",
        "            if not text.strip():\n",
        "                print(\"   ‚ö†Ô∏è OCR returned empty text.\")\n",
        "                continue\n",
        "\n",
        "            chunks = text_splitter.split_documents([Document(page_content=text, metadata={})])\n",
        "            filtered_chunks = []\n",
        "\n",
        "            for chunk in chunks:\n",
        "                chunk_hash = compute_hash(chunk.page_content)\n",
        "                if chunk_hash not in existing_hashes:\n",
        "                    chunk.metadata = {\n",
        "                        \"source\": path,\n",
        "                        \"file_name\": file,\n",
        "                        \"content_hash\": chunk_hash,\n",
        "                        \"section\": \"ocr_recovered\"\n",
        "                    }\n",
        "                    filtered_chunks.append(chunk)\n",
        "\n",
        "            if filtered_chunks:\n",
        "                print(f\"   ‚ûï {len(filtered_chunks)} new chunks to add.\")\n",
        "                new_chunks.extend(filtered_chunks)\n",
        "\n",
        "                if not preview_shown:\n",
        "                    print(\"\\nüîç OCR Preview:\")\n",
        "                    display(Markdown(f\"**File:** `{file}`\"))\n",
        "                    display(Markdown(f\"**Metadata:** `{filtered_chunks[0].metadata}`\"))\n",
        "                    print(filtered_chunks[0].page_content[:1000])\n",
        "                    preview_shown = True\n",
        "            else:\n",
        "                print(\"   ‚ÑπÔ∏è All OCR chunks already exist in Chroma.\")\n",
        "\n",
        "    if new_chunks:\n",
        "        print(f\"\\nüöÄ Adding {len(new_chunks)} new OCR-recovered chunks...\")\n",
        "        chroma_store.add_documents(new_chunks)\n",
        "        chroma_store.persist()\n",
        "        print(\"‚úÖ Chroma DB updated successfully!\")\n",
        "    else:\n",
        "        print(\"‚úÖ No new OCR chunks to add.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "81Nztok-G_uX",
        "outputId": "067cf048-e813-4564-f5ae-0db73f268a45"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting ocr_update.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ezFCPWgwcY9C",
        "outputId": "5707dbfb-3b6b-49e9-85fb-52c920245d84"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üöÄ Starting vector store initialization/update process\n",
            "üóÇÔ∏è  Using dynamic collection name: TEESTA_PROJECT\n",
            "üîß Initializing embedding model...\n",
            "‚úÖ Embedding model loaded.\n",
            "üîå Connecting to Chroma DB...\n",
            "üìö Found 13 existing collections: ['Health_Insurance_Young_Star_Insurance', 'Sample_PDF_1', 'Resume', 'IRDA', 'Adani', 'Care_Supreme', 'Tender', 'Sample_PDF_2', 'TEESTA_PROJECT', '2023-24___Annual_Report_2023-24-199-392_compressedpdf', 'publication_battery220_0001_removed_10_03pdf', 'BTS600', 'IS1652']\n",
            "üì¶ Found existing collection: TEESTA_PROJECT\n",
            "üîÑ Loading existing Chroma collection: TEESTA_PROJECT...\n",
            "üîÑ Updating Chroma vector store...\n",
            "üîÑ Pulling existing content hashes from Chroma...\n",
            "üìä Found 45 existing hashes.\n",
            "üì• Loading new documents from: /content/drive/MyDrive/TEESTA PROJECT\n",
            "üìÑ Loaded 0 raw documents.\n",
            "‚ö†Ô∏è No documents found to process.\n",
            "üåà Vector store operation completed successfully!\n",
            "‚ùå No metadata found. Executing OCR processing script...\n",
            "üóÇ Using collection name: `TEESTA_PROJECT`\n",
            "üîç Initializing embedding and vector store...\n",
            "üîé Scanning and applying OCR to all PDFs...\n",
            "‚ñ∂Ô∏è [1] Processing: publication battery220_0001_removed_10_03.pdf\n",
            "   ‚ÑπÔ∏è All OCR chunks already exist in Chroma.\n",
            "‚úÖ No new OCR chunks to add.\n"
          ]
        }
      ],
      "source": [
        "####################################################################\n",
        "# üåüüåüüåüüåüüåü Optimized Vector Store Script with Enhanced Process Messaging üåüüåüüåüüåüüåü\n",
        "####################################################################\n",
        "#                                                                  #\n",
        "# This script efficiently manages document storage and updates in #\n",
        "# a Chroma vector store. It handles the following tasks:           #\n",
        "#                                                                  #\n",
        "# - üì• Load documents from multiple formats (text, PDF)            #\n",
        "# - üîë Compute unique content hashes to prevent duplication       #\n",
        "# - ‚úÇÔ∏è Split documents into smaller chunks for efficient storage  #\n",
        "# - üîÑ Update Chroma vector store with new documents               #\n",
        "# - ‚ö° Create or update a Chroma vector store in an existing DB    #\n",
        "#                                                                  #\n",
        "# With this script, you‚Äôll have an optimized and scalable          #\n",
        "# solution for managing and searching your document vectors.       #\n",
        "###########################################1#########################\n",
        "# üíª Enjoy smooth vector store operations and seamless document    #@\n",
        "# management! üåê                                                  #\n",
        "####################################################################\n",
        "\n",
        "# Optimized Vector Store Script with Enhanced Process Messaging\n",
        "import os\n",
        "import re\n",
        "import hashlib\n",
        "import chromadb\n",
        "from langchain.document_loaders import TextLoader, PyPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from ocr_update import ocr_and_update_chroma  # Importing the second script function\n",
        "\n",
        "\n",
        "# Add this flag at the top of your script\n",
        "metadata_created = False\n",
        "\n",
        "def compute_hash(content: str) -> str:\n",
        "    \"\"\"Compute SHA-256 hash of the document content.\"\"\"\n",
        "    #...... Computing hash for document content\n",
        "    return hashlib.sha256(content.encode('utf-8')).hexdigest()\n",
        "\n",
        "def get_existing_hashes(chroma_store) -> set:\n",
        "    existing_hashes = set()\n",
        "    try:\n",
        "        results = chroma_store._collection.get(include=[\"metadatas\"])\n",
        "        metadatas = results.get(\"metadatas\", [])\n",
        "        for metadata in metadatas:\n",
        "            if metadata and \"content_hash\" in metadata:\n",
        "                existing_hashes.add(metadata[\"content_hash\"])\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Error retrieving existing hashes from Chroma: {str(e)}\")\n",
        "    return existing_hashes\n",
        "\n",
        "def extract_clean_metadata(raw_metadata, file_path):\n",
        "    cleaned = {\n",
        "        \"source\": file_path,\n",
        "        \"file_name\": os.path.basename(file_path)\n",
        "    }\n",
        "    if raw_metadata:\n",
        "        for key, value in raw_metadata.items():\n",
        "            # Only include serializable and clean fields\n",
        "            if isinstance(key, str) and isinstance(value, (str, int, float)):\n",
        "                cleaned[key] = str(value)\n",
        "    return cleaned\n",
        "\n",
        "def load_documents_from_drive(docs_path: str):\n",
        "    \"\"\"Load documents with clean, reliable PDF metadata handling\"\"\"\n",
        "    docs = []\n",
        "    for root, _, files in os.walk(docs_path):\n",
        "        for file in files:\n",
        "            file_path = os.path.join(root, file)\n",
        "            if file.endswith(\".pdf\"):\n",
        "                try:\n",
        "                    raw_reader = PdfReader(file_path)\n",
        "                    pdf_metadata = extract_clean_metadata(raw_reader.metadata, file_path)\n",
        "\n",
        "                    for page_num, page in enumerate(raw_reader.pages):\n",
        "                        text = page.extract_text()\n",
        "                        if text and text.strip():  # Avoid empty pages\n",
        "                            docs.append(Document(\n",
        "                                page_content=text,\n",
        "                                metadata={**pdf_metadata, \"page\": page_num}\n",
        "                            ))\n",
        "                except Exception as e:\n",
        "                    print(f\"‚ö†Ô∏è Error loading {file_path}: {str(e)}\")\n",
        "            elif file.endswith(\".txt\"):\n",
        "                loader = TextLoader(file_path)\n",
        "                docs.extend(loader.load())\n",
        "    return docs\n",
        "\n",
        "def split_and_prepare_documents(docs, chunk_size: int = 1000, chunk_overlap: int = 200):\n",
        "    \"\"\"Split documents into smaller chunks for Chroma.\"\"\"\n",
        "    #...... Initializing document splitter (chunk_size={chunk_size}, overlap={chunk_overlap})\n",
        "    text_splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=chunk_size,\n",
        "        chunk_overlap=chunk_overlap\n",
        "    )\n",
        "    #...... Splitting {len(docs)} documents into chunks\n",
        "    all_splits = text_splitter.split_documents(docs)\n",
        "    return all_splits\n",
        "\n",
        "def initialize_embeddings():\n",
        "    \"\"\"Initialize HuggingFace Embeddings.\"\"\"\n",
        "    #...... Loading sentence-transformers/all-mpnet-base-v2 model\n",
        "    print(\"üîß Initializing embedding model...\")\n",
        "    embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\n",
        "    print(\"‚úÖ Embedding model loaded.\")\n",
        "    return embeddings\n",
        "\n",
        "def connect_to_chroma(persist_dir):\n",
        "    \"\"\"Connect to a Chroma PersistentClient.\"\"\"\n",
        "    #...... Attempting connection to Chroma at {persist_dir}\n",
        "    print(\"üîå Connecting to Chroma DB...\")\n",
        "    try:\n",
        "        client = chromadb.PersistentClient(path=persist_dir)\n",
        "        collections_info = client.list_collections()\n",
        "        collection_names = [col if isinstance(col, str) else col[\"name\"] for col in collections_info]\n",
        "        print(f\"üìö Found {len(collection_names)} existing collections: {collection_names}\")\n",
        "        return client, collection_names\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Error connecting to Chroma: {str(e)}\")\n",
        "        return None, []\n",
        "\n",
        "def load_existing_chroma_store(persist_dir, embeddings, collection_name):\n",
        "    \"\"\"Load existing Chroma vector store.\"\"\"\n",
        "    #...... Loading collection '{collection_name}' with embeddings\n",
        "    print(f\"üîÑ Loading existing Chroma collection: {collection_name}...\")\n",
        "    return Chroma(\n",
        "        client=chromadb.PersistentClient(path=persist_dir),\n",
        "        collection_name=collection_name,\n",
        "        embedding_function=embeddings,\n",
        "        persist_directory=persist_dir\n",
        "    )\n",
        "\n",
        "def create_new_vector_store(embeddings, persist_dir: str, docs_path: str, collection_name: str):\n",
        "    \"\"\"Create a brand new Chroma vector store.\"\"\"\n",
        "    #...... Starting new vector store creation process\n",
        "    print(\"üì• Loading documents from drive...\")\n",
        "    docs = load_documents_from_drive(docs_path)\n",
        "    print(f\"üìÑ Loaded {len(docs)} documents.\")\n",
        "\n",
        "    print(\"‚úÇÔ∏è Splitting documents into chunks...\")\n",
        "    all_splits = split_and_prepare_documents(docs)\n",
        "    print(f\"üß© Split into {len(all_splits)} chunks.\")\n",
        "\n",
        "    #...... Adding metadata (hashes) to all chunks\n",
        "    for doc in all_splits:\n",
        "        doc.metadata[\"content_hash\"] = compute_hash(doc.page_content)\n",
        "        doc.metadata[\"section\"] = \"all_sections\"\n",
        "        # Check if metadata is successfully created\n",
        "        if doc.metadata:\n",
        "          metadata_created = True  # Set the flag to True if metadata is created\n",
        "\n",
        "    print(\"üõ†Ô∏è Creating new Chroma DB...\")\n",
        "    chroma_store = Chroma.from_documents(\n",
        "        documents=all_splits,\n",
        "        embedding=embeddings,\n",
        "        collection_name=collection_name,\n",
        "        persist_directory=persist_dir\n",
        "    )\n",
        "    print(\"‚úÖ New Chroma DB created and saved successfully!\")\n",
        "    return chroma_store\n",
        "\n",
        "def update_vectorstore(\n",
        "    chroma_store: Chroma,\n",
        "    new_docs_path: str,\n",
        "    chunk_size: int = 1000,\n",
        "    chunk_overlap: int = 200\n",
        "):\n",
        "    \"\"\"Update the Chroma vector store with new documents.\"\"\"\n",
        "    #...... Starting vector store update process\n",
        "    print(\"üîÑ Pulling existing content hashes from Chroma...\")\n",
        "    existing_hashes = get_existing_hashes(chroma_store)\n",
        "    print(f\"üìä Found {len(existing_hashes)} existing hashes.\")\n",
        "\n",
        "    print(f\"üì• Loading new documents from: {new_docs_path}\")\n",
        "    raw_docs = load_documents_from_drive(new_docs_path)\n",
        "    print(f\"üìÑ Loaded {len(raw_docs)} raw documents.\")\n",
        "\n",
        "    if not raw_docs:\n",
        "        print(\"‚ö†Ô∏è No documents found to process.\")\n",
        "        return chroma_store\n",
        "\n",
        "    #...... Splitting documents into chunks first\n",
        "    print(\"‚úÇÔ∏è Splitting documents into chunks...\")\n",
        "    split_docs = split_and_prepare_documents(raw_docs, chunk_size, chunk_overlap)\n",
        "    print(f\"üß© Split into {len(split_docs)} chunks.\")\n",
        "\n",
        "    #...... Preparing and filtering chunks\n",
        "    unique_chunks = []\n",
        "    for chunk in split_docs:\n",
        "        chunk_hash = compute_hash(chunk.page_content)\n",
        "        chunk.metadata[\"content_hash\"] = chunk_hash\n",
        "        chunk.metadata[\"section\"] = \"all_sections\"\n",
        "\n",
        "        if chunk_hash not in existing_hashes:\n",
        "            unique_chunks.append(chunk)\n",
        "\n",
        "    print(f\"‚úÖ Found {len(unique_chunks)} new unique chunks to add.\")\n",
        "\n",
        "    if not unique_chunks:\n",
        "        print(\"‚ö†Ô∏è No new unique chunks found. Vector store is already up-to-date.\")\n",
        "        return chroma_store\n",
        "\n",
        "    print(\"‚ûï Adding new unique chunks to Chroma...\")\n",
        "    chroma_store.add_documents(unique_chunks)\n",
        "    chroma_store.persist()\n",
        "    print(f\"üéâ Successfully added {len(unique_chunks)} new chunks and persisted.\")\n",
        "\n",
        "    return chroma_store\n",
        "\n",
        "def initialize_or_update_vector_store(persist_dir: str, docs_path: str, collection_name: str = None):\n",
        "    \"\"\"Main function to initialize or update the vector store.\"\"\"\n",
        "    #...... Starting vector store initialization\n",
        "\n",
        "    # ‚úÖ Insert sanitize_collection_name here\n",
        "    def sanitize_collection_name(name: str) -> str:\n",
        "        \"\"\"Sanitize the folder name to a valid Chroma collection name.\"\"\"\n",
        "        name = name.replace(\" \", \"_\")\n",
        "        name = re.sub(r'[^a-zA-Z0-9_\\-]', '', name)\n",
        "        name = name.strip(\"_-\")\n",
        "        name = name[:63]\n",
        "        return name\n",
        "\n",
        "    if collection_name is None:\n",
        "        collection_name = os.path.basename(docs_path)\n",
        "        collection_name = sanitize_collection_name(collection_name)\n",
        "        print(f\"üóÇÔ∏è  Using dynamic collection name: {collection_name}\")\n",
        "\n",
        "    #...... Initializing embedding model\n",
        "    embeddings = initialize_embeddings()\n",
        "\n",
        "    #...... Connecting to Chroma DB\n",
        "    client, collections = connect_to_chroma(persist_dir)\n",
        "\n",
        "    if collections:\n",
        "        if collection_name in collections:\n",
        "            #...... Existing collection workflow\n",
        "            print(f\"üì¶ Found existing collection: {collection_name}\")\n",
        "            chroma_store = load_existing_chroma_store(persist_dir, embeddings, collection_name)\n",
        "            print(\"üîÑ Updating Chroma vector store...\")\n",
        "            update_vectorstore(chroma_store, docs_path)\n",
        "            return chroma_store\n",
        "        else:\n",
        "            #...... New collection in existing DB\n",
        "            print(f\"‚ö° No matching collection found. Creating new collection: {collection_name}\")\n",
        "            chroma_store = create_new_vector_store(embeddings, persist_dir, docs_path, collection_name)\n",
        "            return chroma_store\n",
        "\n",
        "    if os.path.exists(persist_dir):\n",
        "        #...... New collection in existing directory\n",
        "        print(f\"‚ö° Chroma DB found at {persist_dir} but no collection exists.\")\n",
        "        chroma_store = create_new_vector_store(embeddings, persist_dir, docs_path, collection_name)\n",
        "        return chroma_store\n",
        "\n",
        "    #...... Fresh installation workflow\n",
        "    print(\"‚ùå No existing Chroma DB found. Creating new vector store...\")\n",
        "    chroma_store = create_new_vector_store(embeddings, persist_dir, docs_path, collection_name)\n",
        "    return chroma_store\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    #...... Starting script execution\n",
        "    persist_dir = \"/content/drive/MyDrive\"\n",
        "    docs_path = \"/content/drive/MyDrive/TEESTA PROJECT\"\n",
        "\n",
        "    #...... Initializing/updating vector store\n",
        "    print(\"üöÄ Starting vector store initialization/update process\")\n",
        "    chroma_store = initialize_or_update_vector_store(persist_dir, docs_path)\n",
        "    print(\"üåà Vector store operation completed successfully!\")\n",
        "\n",
        "    # After all processing in the first script, add this check:\n",
        "if not metadata_created:\n",
        "    print(\"‚ùå No metadata found. Executing OCR processing script...\")\n",
        "    # Now, call the OCR script from the second script\n",
        "    ocr_and_update_chroma(docs_path, persist_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oy7vgfGOC9JY"
      },
      "outputs": [],
      "source": [
        "####################################################################################################\n",
        "# üåêüîç Unified Search Across All Chroma Collections with Enhanced Query Results üéØ                #\n",
        "####################################################################################################\n",
        "#                                                                                                #\n",
        "# This script allows you to perform a powerful and efficient search across multiple collections    #\n",
        "# stored in Chroma, without loading all documents into memory. The process is designed to:         #\n",
        "#                                                                                                #\n",
        "# - üß† Search across multiple Chroma collections using similarity-based queries.                   #\n",
        "# - ‚öôÔ∏è Filter and rank results based on a specific section or general search parameters.           #\n",
        "# - üîç Retrieve and merge the top-k most relevant results based on similarity score.               #\n",
        "# - üîÑ Process each collection individually to avoid memory overload and optimize performance.      #\n",
        "#                                                                                                #\n",
        "# Perfect for scenarios where you need to search through large datasets and retrieve only the      #\n",
        "# most relevant documents efficiently.                                                             #\n",
        "#                                                                                                #\n",
        "# Ready to unlock the full potential of your Chroma-based vector store? Let's dive into search!     #\n",
        "####################################################################################################\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "from langchain.vectorstores import Chroma\n",
        "import chromadb\n",
        "\n",
        "def unified_search(client, persist_dir, embeddings, query: str, k=10, filter_section=\"all_sections\"):\n",
        "    \"\"\"Search across all collections without loading everything into memory.\"\"\"\n",
        "\n",
        "    collection_names = client.list_collections()\n",
        "    print(\"üì¶ Available collections:\")\n",
        "    for name in collection_names:\n",
        "      print(f\" - {name if isinstance(name, str) else name.get('name')}\")\n",
        "\n",
        "    all_results = []\n",
        "\n",
        "    print(f\"üîç Searching across {len(collection_names)} collections...\")\n",
        "\n",
        "    for collection_name in collection_names:\n",
        "        if isinstance(collection_name, dict):\n",
        "            collection_name = collection_name.get(\"name\")\n",
        "\n",
        "        print(f\"‚û°Ô∏è Searching in collection: {collection_name}\")\n",
        "\n",
        "        chroma_store = Chroma(\n",
        "            client=client,\n",
        "            collection_name=collection_name,\n",
        "            embedding_function=embeddings,\n",
        "            persist_directory=persist_dir\n",
        "        )\n",
        "\n",
        "        try:\n",
        "            results = chroma_store.similarity_search_with_score(\n",
        "                query,\n",
        "                k=k,\n",
        "                filter= None\n",
        "            )\n",
        "            all_results.extend(results)\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è Error searching collection {collection_name}: {str(e)}\")\n",
        "            continue\n",
        "\n",
        "    print(f\"üîé Total retrieved across all collections: {len(all_results)}\")\n",
        "\n",
        "    # Sort all results by similarity score (lower score = more similar)\n",
        "    all_results.sort(key=lambda x: x[1])\n",
        "\n",
        "    # Take top k\n",
        "    top_results = all_results[:k]\n",
        "\n",
        "    retrieved_docs = [doc for doc, score in top_results]\n",
        "\n",
        "    print(f\"‚úÖ Found {len(retrieved_docs)} total documents after merging.\")\n",
        "    return retrieved_docs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nAS2kVvVYgWZ"
      },
      "outputs": [],
      "source": [
        "####_________________________________________#####\n",
        "# üåç Unified Search Across All Chroma Collections üöÄ\n",
        "# ‚ú® Effortless Querying, One Search to Rule Them All üîç\n",
        "# üî• Fast, Smart, and Scalable Search Across Data üß†\n",
        "####_________________________________________#####\n",
        "\n",
        "\n",
        "\n",
        "#This RAG pipeline uses the Chroma DB store for faster retrieval\n",
        "\n",
        "# Define the RAG pipeline components (unchanged from your original)\n",
        "class Search(TypedDict):\n",
        "    \"\"\"Search query.\"\"\"\n",
        "    query: Annotated[str, ..., \"Search query to run.\"]\n",
        "    section: Annotated[\n",
        "        Literal[\"all_sections\"],\n",
        "        ...,\n",
        "        \"Section to query.\",\n",
        "    ]\n",
        "\n",
        "prompt = hub.pull(\"rlm/rag-prompt\")\n",
        "\n",
        "class State(TypedDict):\n",
        "    question: str\n",
        "    query: Search\n",
        "    context: List[Document]\n",
        "    answer: str\n",
        "\n",
        "# Modified RAG functions to work with dual stores\n",
        "def analyze_query(state: State):\n",
        "    structured_llm = llm.with_structured_output(Search)\n",
        "    query = structured_llm.invoke(state[\"question\"])\n",
        "    return {\"query\": query}\n",
        "\n",
        "def retrieve(state: State):\n",
        "    query = state[\"query\"]\n",
        "    retrieved_docs = unified_search(\n",
        "        client,\n",
        "        persist_dir,\n",
        "        embeddings,\n",
        "        query=query[\"query\"],\n",
        "        k=4,\n",
        "        filter_section=query[\"section\"]\n",
        "    )\n",
        "    return {\"context\": retrieved_docs}\n",
        "\n",
        "\n",
        "def generate(state: State):\n",
        "    time.sleep(1)  # Rate limit protection\n",
        "    docs_content = \"\\n\\n\".join(doc.page_content for doc in state[\"context\"])\n",
        "    messages = prompt.invoke({\"question\": state[\"question\"], \"context\": docs_content})\n",
        "    response = llm.invoke(messages)\n",
        "    return {\"answer\": response.content}\n",
        "\n",
        "# Build the execution graph\n",
        "graph_builder = StateGraph(State).add_sequence([analyze_query, retrieve, generate])\n",
        "graph_builder.add_edge(START, \"analyze_query\")\n",
        "graph = graph_builder.compile()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mPvWtxj5FtiV"
      },
      "outputs": [],
      "source": [
        "persist_dir = \"/content/drive/MyDrive\"\n",
        "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\n",
        "client = chromadb.PersistentClient(path=persist_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cq3KPmLZYq4-",
        "outputId": "2f36e484-4bd5-4162-cec2-1356f55268c2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üì¶ Available collections:\n",
            " - Health_Insurance_Young_Star_Insurance\n",
            " - Sample_PDF_1\n",
            " - Resume\n",
            " - IRDA\n",
            " - Adani\n",
            " - Care_Supreme\n",
            " - Tender\n",
            " - Sample_PDF_2\n",
            " - TEESTA_PROJECT\n",
            " - 2023-24___Annual_Report_2023-24-199-392_compressedpdf\n",
            " - publication_battery220_0001_removed_10_03pdf\n",
            " - BTS600\n",
            " - IS1652\n",
            "üîç Searching across 13 collections...\n",
            "‚û°Ô∏è Searching in collection: Health_Insurance_Young_Star_Insurance\n",
            "‚û°Ô∏è Searching in collection: Sample_PDF_1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:chromadb.segment.impl.vector.local_persistent_hnsw:Number of requested results 4 is greater than number of elements in index 1, updating n_results = 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚û°Ô∏è Searching in collection: Resume\n",
            "‚û°Ô∏è Searching in collection: IRDA\n",
            "‚û°Ô∏è Searching in collection: Adani\n",
            "‚û°Ô∏è Searching in collection: Care_Supreme\n",
            "‚û°Ô∏è Searching in collection: Tender\n",
            "‚û°Ô∏è Searching in collection: Sample_PDF_2\n",
            "‚û°Ô∏è Searching in collection: TEESTA_PROJECT\n",
            "‚û°Ô∏è Searching in collection: 2023-24___Annual_Report_2023-24-199-392_compressedpdf\n",
            "‚û°Ô∏è Searching in collection: publication_battery220_0001_removed_10_03pdf\n",
            "‚û°Ô∏è Searching in collection: BTS600\n",
            "‚û°Ô∏è Searching in collection: IS1652\n",
            "üîé Total retrieved across all collections: 41\n",
            "‚úÖ Found 4 total documents after merging.\n",
            "Bidders must have a Minimum Average Annual Turnover (MAAT) of Rs. 12,00,000 for the best year out of the last five financial years. Required documents include a valid GST Registration certificate, GSTR3B or Annual return-GSTR9 for the last two years, PAN Card, and audited annual accounts or IT returns for the last three years. Dealers must provide documentary evidence from the OEM to meet certain criteria.\n"
          ]
        }
      ],
      "source": [
        "# RAG pipeline remains unchanged\n",
        "def ask_question(question: str):\n",
        "    result = graph.invoke({\"question\": question})\n",
        "    return result[\"answer\"]\n",
        "\n",
        "# Usage example\n",
        "if __name__ == \"__main__\":\n",
        "    # No initialization here - just usage\n",
        "    print(ask_question(\"What are the specific requirements for bidders, including documentation and minimum average annual turnover for 220V 300Ah PLANTE Stationery battery?\"))\n",
        "    #print(ask_question(\"What are the exclusions?\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New Section"
      ],
      "metadata": {
        "id": "lEpYISZRZBbg"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1I9kw0L7LDxUgds8d1nUBLek8Ef8gKYyQ",
      "authorship_tag": "ABX9TyOSJPFVmlwFV4EXUB4WOwL0",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}